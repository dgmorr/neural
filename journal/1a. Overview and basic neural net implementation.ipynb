{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project overview\n",
    "## What are you doing?\n",
    "I'm trying to implement some common deep learning architectures from the ground up, starting with a simple feedforward neural net and hopefully moving on to some more interesting structures like long short-term memory models. By \"from the ground up\", I mean using as few existing libraries as possible and writing my own code without reference to prior implementations. I'm also documenting this process as I go.\n",
    "## Ok but why are you doing that\n",
    "I'm trying to get a better handle on the fundamentals before I start in on projects using more complicated architectures. I know how a neural net operates at a fairly high level, but I don't use anything ML-related in my day job and it's not something I focused on much in college. I want to prove to myself that I really understand how, say, backprogation works during the training process, in hopes that this will help me develop better intuitions about more complex structures. I'm not aiming for some highly efficient deep learning library here; I just want a toy net that can classify iris species or whatever.\n",
    "\n",
    "The documentation part is mainly for my own benefit, so that I have a record of what I've figured out and what I want to try next. However, if you're reading this and you're not me, cool! I'm planning on writing this as if for a (small, informal) audience, and maybe it'll be helpful to someone else down the road. Caveat: I'm not an authoritative source on any of this and I will probably make some misleading/fully incorrect statements as I figure things out, so please take things with a grain of salt and double-check any math I'm putting in here. I'll link sources that I find helpful as I go.\n",
    "## What's the plan?\n",
    "Based on my limited research so far, I'm thinking something like feedforward nets -> simple RNNs -> LSTMs -> transformers, with various detours along the way depending on what I find out. I'm mainly interested in language rather than other applications, so CNNs are off the list for now. I haven't heard a ton about GANs for language applications either.\n",
    "\n",
    "I don't expect that any of my implementations of architectures beyond simple feedforward will be able to handle any real training in a way that is efficient enough for actual use, so for any projects using those architectures I'll use something that actually works - I'll build my own slow, shitty version to understand what's happening, then use the pytorch version in an application.\n",
    "## What's the end goal?\n",
    "To know more? I guess? Ideally this will give me a better foundation for larger projects. I've been kicking around the idea of applying a generative language architecture to a corpus of music scores and trying out AI-assisted composition, but right now I don't really have enough know-how to set that up properly, and I don't want to just follow some out-of-the-box model. I'd also like to be more informed about new developments in AI, which are happening at an alarming rate these days.\n",
    "## What level of knowledge are you starting with, exactly?\n",
    "CS degree, decent math background, and 3 1/2 years of non-ML-related industry experience. The CS degree included some courses that collectively spent maybe a week on neural nets (including calculating a bunch of training example gradients by hand) and several more weeks on other ML topics (from linear regression to support vector machines). I was very linguistics-focused in undergrad, so I also spent quite a bit of time on practical NLP and traditional/computational linguistics. A lot of this is out of date, though, so I anticipate relearning most of this to catch up with state of the art. I'm essentially a data engineer in my day job, and I have a good amount of experience with big data processing, although not at the scale that modern models would need.\n",
    "## Sounds good! Get started!\n",
    "yeah yeah yeah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a Feedforward neural net structure\n",
    "Things I remember about how neural networks are set up:\n",
    "* One net is composed of two or more layers, each of which is composed of some number of neurons\n",
    "* Each neuron in one layer is connected to all the neurons in the next layer\n",
    "* Each connection has a weight; each neuron has a bias (disclosure: I forgot about the bias originally and had to go back and add it after the fact)\n",
    "* Each neuron has a particular activation function\n",
    "* Output of a starting layer neuron is just the input value; for subsequent layers, output = weighted sum of inputs, plus the bias, passed through the activation function\n",
    "\n",
    "So let's start at the bottom with a Neuron object. (Yes, I know that, in practice, a neural net is basically just an alternating chain of matrix multiplications and elementwise vector operations. Humor me while I make the nominal structure extremely explicit.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    \n",
    "    # Initialize with an activation function\n",
    "    # Bias, weights, and links will be determined later\n",
    "    def __init__(self, f, b):\n",
    "        self.activation = f\n",
    "        self.bias = b\n",
    "        self.upstream_neurons = []\n",
    "        self.upstream_weights = []\n",
    "        self.downstream_neurons = []\n",
    "        self.downstream_weights = []\n",
    "        self.current_value = None\n",
    "\n",
    "    # Link from an earlier neuron\n",
    "    # Input array and weight array are paired\n",
    "    def link_input(self, input_neuron, weight):\n",
    "        self.upstream_neurons.append(input_neuron)\n",
    "        self.upstream_weights.append(weight)\n",
    "        input_neuron.downstream_neurons.append(self)\n",
    "        input_neuron.downstream_weights.append(weight)\n",
    "\n",
    "    # Link to a later neuron\n",
    "    # Should use one or the other of the link functions for any given net\n",
    "    # to avoid double binding neurons\n",
    "    def link_output(self, output_neuron, weight):\n",
    "        self.downstream_neurons.append(output_neuron)\n",
    "        self.downstream_weights.append(weight)\n",
    "        output_neuron.upstream_neurons.append(self)\n",
    "        output_neuron.upstream_weights.append(weight)\n",
    "        \n",
    "    def set_bias(self, b):\n",
    "        self.bias = b\n",
    "        \n",
    "    def set_value(self, v):\n",
    "        self.current_value = v\n",
    "        \n",
    "    def activate(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.upstream_neurons)):\n",
    "            neuron = self.upstream_neurons[i]\n",
    "            weight = self.upstream_weights[i]\n",
    "            total += neuron.current_value * weight\n",
    "        total += self.bias\n",
    "        self.current_z = total\n",
    "        total = self.activation(total)\n",
    "        self.current_value = total\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need an activation function to instantiate this and I don't remember the formulas for any of the standard ones, but fortunately [this TDS article](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6) has a good table of popular activations (plus their derivatives, which I know are going to be useful for training later). I'm being pretty traditional here, so let's use sigmoid to start out, but I'll put in a few others for later. I'm including an explicit \"identity\" function as well, since that will be the effective activation function of any input layer neurons (ie. function that doesn't do anything to the input values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.e ** (-x))\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def tanh(x):\n",
    "    z = math.e ** 2*x\n",
    "    return (z - 1) / (z + 1)\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Put these in a lookup table that we can use when building nets\n",
    "activation_types = {\n",
    "    \"identity\": identity,\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"tanh\": tanh,\n",
    "    \"relu\": relu\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I've already had to do my first import, but in my defense it's only so I can get a good value of *e* without hardcoding it somewhere. Anyway though now we can instantiate a neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Neuron(sigmoid, 0)\n",
    "n.activation(0)  # sigmoid(0) = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think a \"Layer\" class would be overengineering here (says the one who's building a bad neural net from  scratch) since there's not much additional structure that isn't better captured in either a neuron or in a complete net. There's maybe cause to introduce a few neuron subclasses (e.g. SigmoidNeuron, TanhNeuron) with set activations, but let's do that later if it seems useful. Moving on to the Net class, which is going to be defined as a set of layers, each of which has a size and an activation function (maybe some architectures I don't know about use a variety of activations within a layer, but we'll ignore that for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.input_size = 0\n",
    "        self.output_size = 0\n",
    "        self.n_layers = 0\n",
    "\n",
    "    def _add_layer(self, n, f):\n",
    "        self.layers.append([Neuron(f, 0) for _ in range(n)])\n",
    "\n",
    "    def _fully_connect_layers(self, l1, l2, weights):\n",
    "        for i in range(len(l1)):\n",
    "            for j in range(len(l2)):\n",
    "                input_neuron = l1[i]\n",
    "                output_neuron = l2[j]\n",
    "                weight = weights[j][i]\n",
    "                input_neuron.link_output(output_neuron, weight)\n",
    "\n",
    "    # Spec of the format [(\"identity|sigmoid|tanh|relu\", n)]\n",
    "    # Initialize a neural net with all weights 1 and all biases 0 \n",
    "    def build_from_spec(self, spec):\n",
    "        for (activation, n) in spec:\n",
    "            if activation not in activation_types:\n",
    "                print(f\"Unrecognized activation function: {activation}\")\n",
    "                return\n",
    "            ac = activation_types[activation]\n",
    "            self._add_layer(n, ac)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            l1 = self.layers[i]\n",
    "            l2 = self.layers[i + 1]\n",
    "            n = len(l1)\n",
    "            m = len(l2)\n",
    "            weights = [[1 for _ in range(n)] for _ in  range(m)]\n",
    "            self._fully_connect_layers(l1, l2, weights)\n",
    "        self.input_size = len(self.layers[0])\n",
    "        self.output_size = len(self.layers[-1])\n",
    "        self.n_layers =  len(self.layers)\n",
    "\n",
    "\n",
    "    # input_vector is just a list-like object that can be indexed\n",
    "    def passthrough(self, input_vector):\n",
    "        if len(input_vector) != self.input_size:\n",
    "            print(f\"Mismatched input: expected size {self.input_size} but got size {len(input_vector)}\")\n",
    "        for i in range(self.input_size):\n",
    "            self.layers[0][i].set_value(input_vector[i])\n",
    "        for layer in self.layers[1:]:\n",
    "            for neuron in layer:\n",
    "                neuron.activate()\n",
    "        output_vector = [neuron.current_value for neuron in self.layers[-1]]\n",
    "        return output_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know weights and biases are randomly initialized, but I'm not sure what range/distribution those values are drawn from, so for now weights default to 1 and biases default to 0.\n",
    "\n",
    "Trying it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9525741268224331, 0.9525741268224331, 0.9525741268224331]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNet()\n",
    "spec = [(\"identity\", 4), (\"sigmoid\", 6), (\"sigmoid\", 3)]\n",
    "net.build_from_spec(spec)\n",
    "\n",
    "assert net.n_layers == 3\n",
    "assert [len(net.layers[i]) for i in range(3)] == [4, 6, 3]\n",
    "\n",
    "net.passthrough([0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like a neural net. Now I have to teach myself how to train it.\n",
    "\n",
    "Some sources I found helpful for this section / easing back into this topic generally:\n",
    "* [Chapter 1](http://neuralnetworksanddeeplearning.com/chap1.html) of Michael Nielsen's excellent introductory reference\n",
    "* 3Blue1Brown's (quite concise) [deep learning playlist](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=1), which also covers backpropagation at a high level\n",
    "* Mentioned earlier, Sagar Sharma's [TDS piece] on common activation functions, including formulas for derivatives that will come in handy pretty soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
