{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c16e5f0",
   "metadata": {},
   "source": [
    "# 1c. Training procedures\n",
    "\n",
    "Now that I've got some backpropagation code, I can start training my net and check that it's actually learning. So the order of the day is:\n",
    "\n",
    "1. Fix net initialization process to use random weights and biases.\n",
    "2. Implement a general training procedure that takes a dataset, some training parameters (batch size, etc.) and the neural net to be trained, and applies the backpropagation code from the previous journal to train the net.\n",
    "3. Implement a general testing procedure to check how the training went.\n",
    "4. Try this all out on the iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ba0ac",
   "metadata": {},
   "source": [
    "## 1. Initialization\n",
    "\n",
    "Some sources I should have read previously on weight initialization, both from Jason Brownlee / Machine Learning Mastery:\n",
    "* [Why initialize a neural network with random weights?](https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/)\n",
    "* [Weight Initialization for Deep Learning Neural Networks](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/)\n",
    "\n",
    "Some standard methods to implement:\n",
    "* Weights pulled randomly and uniformly from within a small fixed range (e.g. -0.3 to 0.3 or 0 to 1)\n",
    "* Xavier and normalized Xavier - pull from uniform distribution determined by number of inputs to current neuron (good for sigmoid and tanh neurons, supposedly)\n",
    "\n",
    "One very silly thing about my implementation is that since I'm using a graph model rather than a set of matrices, and all the neuron links are bidirectional to make feedforward and backpropagation calculations easier, every weight is represented in to places - weight $w^l_{jk}$ shows up in the upstream weights for neuron $j$ in layer $l$ _and_ in the downstream weights for neuron $k$ in layer $l - 1$. So I have to make sure that when I pick a random value for $w^l_{jk}$, it gets updated in both places. This will also be true when we're adjusting weights during training. To make it a bit easier to follow, I'm going to just define a getter and setter for weight $w^l_{jk}$ and add it to the Network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f17d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path as needed depending on how you're trying to run this\n",
    "import sys\n",
    "sys.path.append('/home/dgmorrison/Projects/neural/src/')\n",
    "from nn import NeuralNet\n",
    "\n",
    "def get_weight(self, l, j, k):\n",
    "    if l <= 0 or l >= len(self.layers):  # Note layer 0 has no weights\n",
    "        return None\n",
    "    layer = self.layers[l]\n",
    "    prev_layer = self.layers[l - 1]\n",
    "    if j < 0 or j >= len(layer) or k < 0 or k >= len(prev_layer):\n",
    "        return None\n",
    "    neuron_l_j = layer[j]\n",
    "    neuron_prev_k = prev_layer[k]\n",
    "    downstream_w = neuron_prev_k.downstream_weights[j]\n",
    "    upstream_w = neuron_l_j.upstream_weights[k]\n",
    "    assert downstream_w == upstream_w\n",
    "    return downstream_w\n",
    "\n",
    "def set_weight(self, l, j, k, val):\n",
    "    if l <= 0 or l >= len(self.layers):  # Note layer 0 has no weights\n",
    "        return None\n",
    "    layer = self.layers[l]\n",
    "    prev_layer = self.layers[l - 1]\n",
    "    if j <= 0 or j >= len(layer) or k <= 0 or k >= len(prev_layer):\n",
    "        return None\n",
    "    neuron_l_j = layer[j]\n",
    "    neuron_prev_k = prev_layer[k]\n",
    "    neuron_prev_k.downstream_weights[j] = val\n",
    "    neuron_l_j.upstream_weights[k] = val\n",
    "    \n",
    "NeuralNet.get_weight = get_weight\n",
    "NeuralNet.set_weight = set_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3547168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Weight-picker functions for the initialization strategies listed above\n",
    "# uniform is second-order to make general initialization consistent (parametrized by lower and upper bounds)\n",
    "# I miss haskell\n",
    "def uniform(alpha, beta):\n",
    "    def f(**kwargs):\n",
    "        return random.uniform(alpha, beta)\n",
    "    return f\n",
    "\n",
    "# xavier looks at the actual net structure\n",
    "def xavier(**kwargs):\n",
    "    layer = kwargs[\"layer\"]\n",
    "    j = kwargs[\"j\"]\n",
    "    neuron = layer[j]\n",
    "    n_inputs = len(neuron.upstream_weights)\n",
    "    alpha = 1 / (n_inputs ** 0.5)\n",
    "    return random.uniform(-alpha, alpha)\n",
    "\n",
    "def norm_xavier(**kwargs):\n",
    "    layer = kwargs[\"layer\"]\n",
    "    j = kwargs[\"j\"]\n",
    "    neuron = layer[j]\n",
    "    n_inputs = len(neuron.upstream.weights)\n",
    "    layer_size = len(layer)\n",
    "    root_six = 2.44949\n",
    "    alpha = root_six / ((n_inputs + layer_size) ** 0.5)\n",
    "    return random.uniform(-alpha, alpha)\n",
    "\n",
    "# General initializer\n",
    "def initialize_weights(net, f):\n",
    "    for l in range(1, len(net.layers)):\n",
    "        prev_layer = net.layers[l-1]\n",
    "        layer = net.layers[l]\n",
    "        for j in range(len(layer)):\n",
    "            for k in range(len(prev_layer)):\n",
    "                w = f(net=net, prev_layer=prev_layer, layer=layer, l=l, j=j, k=k)\n",
    "                net.set_weight(l, j, k, w)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "352c62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_net = NeuralNet()\n",
    "iris_net.build_from_spec([(\"identity\", 4), (\"sigmoid\", 5), (\"sigmoid\", 3)])\n",
    "initialize_weights(iris_net, norm_xavier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc30194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
